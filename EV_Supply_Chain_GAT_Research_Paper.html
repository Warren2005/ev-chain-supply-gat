<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Graph Attention Networks for EV Supply Chain Volatility Prediction</title>
    <style>
        @page {
            size: letter;
            margin: 1in;
        }
        
        * {
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Georgia', 'Times New Roman', serif;
            line-height: 1.6;
            color: #333;
            max-width: 8.5in;
            margin: 0 auto;
            padding: 0.5in;
            background: #fff;
        }
        
        .title-page {
            text-align: center;
            margin-top: 2in;
            page-break-after: always;
        }
        
        h1.paper-title {
            font-size: 24pt;
            font-weight: bold;
            color: #1a1a1a;
            margin-bottom: 0.5in;
            line-height: 1.3;
        }
        
        .authors {
            font-size: 14pt;
            margin: 0.3in 0;
            color: #333;
        }
        
        .affiliation {
            font-size: 12pt;
            color: #666;
            font-style: italic;
            margin: 0.2in 0;
        }
        
        .date {
            font-size: 12pt;
            color: #666;
            margin-top: 0.5in;
        }
        
        h2 {
            font-size: 16pt;
            font-weight: bold;
            color: #2c3e50;
            margin-top: 24pt;
            margin-bottom: 12pt;
            border-bottom: 2px solid #2c3e50;
            padding-bottom: 6pt;
        }
        
        h3 {
            font-size: 13pt;
            font-weight: bold;
            color: #34495e;
            margin-top: 18pt;
            margin-bottom: 10pt;
        }
        
        h4 {
            font-size: 12pt;
            font-weight: bold;
            color: #555;
            margin-top: 14pt;
            margin-bottom: 8pt;
        }
        
        p {
            text-align: justify;
            margin: 0 0 12pt 0;
            font-size: 11pt;
        }
        
        .abstract {
            background: #f8f9fa;
            padding: 20pt;
            border-left: 4pt solid #2c3e50;
            margin: 20pt 0;
            font-style: italic;
        }
        
        .abstract h2 {
            border: none;
            margin-top: 0;
            font-style: normal;
        }
        
        .keywords {
            margin-top: 12pt;
            font-size: 10pt;
        }
        
        .keywords strong {
            font-weight: bold;
        }
        
        .equation {
            background: #fafafa;
            padding: 15pt;
            margin: 15pt 20pt;
            border-left: 3pt solid #3498db;
            font-family: 'Courier New', monospace;
            font-size: 10pt;
            overflow-x: auto;
        }
        
        .equation-number {
            float: right;
            color: #666;
        }
        
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 15pt 0;
            font-size: 10pt;
        }
        
        table thead {
            background: #2c3e50;
            color: white;
            font-weight: bold;
        }
        
        table th, table td {
            border: 1px solid #ddd;
            padding: 8pt;
            text-align: left;
        }
        
        table tbody tr:nth-child(even) {
            background: #f8f9fa;
        }
        
        .results-box {
            background: #e8f4f8;
            border: 2px solid #3498db;
            padding: 15pt;
            margin: 15pt 0;
            border-radius: 4pt;
        }
        
        .finding {
            background: #fff3cd;
            border-left: 4pt solid #ffc107;
            padding: 12pt;
            margin: 12pt 0;
        }
        
        .critical-finding {
            background: #f8d7da;
            border-left: 4pt solid #dc3545;
            padding: 12pt;
            margin: 12pt 0;
        }
        
        .success-finding {
            background: #d4edda;
            border-left: 4pt solid #28a745;
            padding: 12pt;
            margin: 12pt 0;
        }
        
        ul, ol {
            margin: 10pt 0;
            padding-left: 30pt;
        }
        
        li {
            margin: 6pt 0;
        }
        
        .page-break {
            page-break-after: always;
        }
        
        .reference {
            margin-left: 20pt;
            text-indent: -20pt;
            margin-bottom: 8pt;
        }
        
        code {
            background: #f4f4f4;
            padding: 2pt 6pt;
            border-radius: 3pt;
            font-family: 'Courier New', monospace;
            font-size: 9pt;
        }
        
        .caption {
            font-size: 10pt;
            font-style: italic;
            color: #666;
            text-align: center;
            margin-top: 8pt;
        }
        
        .footnote {
            font-size: 9pt;
            color: #666;
            margin-top: 20pt;
            border-top: 1px solid #ddd;
            padding-top: 10pt;
        }
        
        @media print {
            body {
                margin: 0;
                padding: 0;
            }
            .page-break {
                page-break-after: always;
            }
        }
    </style>
</head>
<body>

<!-- TITLE PAGE -->
<div class="title-page">
    <h1 class="paper-title">
        Do Supply Chain Relationships Predict Stock Volatility?<br>
        A Rigorous Test of Graph Attention Networks<br>
        in Financial Markets
    </h1>
    
    <div class="authors">
        Warren L. Davis
    </div>
    
    <div class="affiliation">
        Independent Research<br>
        Electric Vehicle Supply Chain Analysis Project
    </div>
    
    <div class="date">
        December 2024
    </div>
</div>

<!-- ABSTRACT -->
<div class="abstract">
    <h2>Abstract</h2>
    <p>
        <strong>Background:</strong> Graph Neural Networks (GNNs) have shown promise in domains where relationships 
        between entities matter. In financial markets, supply chain relationships connect companies in complex networks 
        that might transmit volatility shocks. This research tests whether incorporating these relationships via Graph 
        Attention Networks (GATs) improves stock volatility prediction compared to traditional temporal models.
    </p>
    
    <p>
        <strong>Methods:</strong> We constructed a knowledge graph of seven Electric Vehicle (EV) supply chain stocks 
        (raw materials: ALB, SQM; components: APTV, MGA; OEMs: F, GM, TSLA) with 15 documented supplier-customer 
        relationships extracted from SEC filings. We developed a Spatio-Temporal Graph Attention Network (ST-GAT) 
        combining graph attention (spatial) and LSTM layers (temporal) with 35,713 parameters. The model was trained 
        on 1,800 samples (2020-2024) with 15 features including GARCH volatility, technical indicators, and 
        macroeconomic variables. Performance was compared against four baselines: GARCH(1,1), VAR(5), Simple LSTM, 
        and Persistence using rigorous statistical testing (paired t-tests, Diebold-Mariano tests, effect sizes).
    </p>
    
    <p>
        <strong>Results:</strong> Contrary to expectations, graph structure significantly <em>harmed</em> prediction 
        performance. SimpleLSTM (no graph) achieved R¬≤=0.67 with 91.2% directional accuracy, while ST-GAT achieved 
        R¬≤=-1.50 with 86.5% directional accuracy. The difference was highly significant (t=3.41, <em>p</em>=0.014, 
        Cohen's <em>d</em>=1.71, large effect). ST-GAT performed well only on upstream stocks (ALB R¬≤=0.86, SQM R¬≤=0.83) 
        but catastrophically failed on downstream OEMs (TSLA R¬≤=-3.75, F R¬≤=-2.74, GM R¬≤=-2.72). Attention analysis 
        revealed the model learned economically meaningful patterns (100% attention to ALB‚ÜíMGA pathway), but these 
        patterns did not translate to improved predictions.
    </p>
    
    <p>
        <strong>Conclusions:</strong> Supply chain relationships, while economically important for long-term 
        dependencies, do not drive short-term stock volatility. Temporal dynamics (momentum, mean reversion) 
        dominate over graph-structured causal relationships. This represents a valuable negative result with 
        strong statistical evidence (large effect size, p<0.05) demonstrating that graph-based approaches can 
        underperform simpler alternatives when structural assumptions don't match domain dynamics. The findings 
        challenge the assumption that incorporating domain knowledge through graph structure always improves 
        machine learning models.
    </p>
    
    <div class="keywords">
        <strong>Keywords:</strong> Graph Attention Networks, Financial Volatility, Supply Chain Networks, 
        Deep Learning, Time Series Forecasting, LSTM, Negative Results
    </div>
</div>

<div class="page-break"></div>

<!-- INTRODUCTION -->
<h2>1. Introduction</h2>

<h3>1.1 Motivation and Background</h3>

<p>
Financial markets exhibit complex interdependencies where shocks propagate across related securities. Traditional 
models like GARCH (Generalized Autoregressive Conditional Heteroscedasticity) and VAR (Vector Autoregression) capture 
temporal dependencies and correlations but ignore structured relationships between companies. The rise of Graph Neural 
Networks (GNNs) offers a potential solution: explicitly model company relationships as a graph where nodes represent 
stocks and edges represent economic linkages.
</p>

<p>
Supply chains provide natural graph structure in equity markets. Raw material suppliers (e.g., lithium miners) connect 
to component manufacturers (e.g., battery producers), who supply original equipment manufacturers (OEMs) like automotive 
companies. Economic theory suggests volatility shocks should propagate along these edges‚Äîa price spike in lithium affects 
battery costs, which impacts car manufacturers' margins and stock prices.
</p>

<p>
Recent advances in Graph Attention Networks (GATs) enable models to learn which connections matter most through 
attention mechanisms. Unlike fixed-weight GNNs, GATs dynamically weight edge importance, potentially identifying 
critical transmission pathways during volatility events. This seems ideally suited to financial markets where 
relationship importance varies over time.
</p>

<h3>1.2 Research Questions</h3>

<p>This research addresses three primary questions:</p>

<ol>
    <li><strong>Performance Question:</strong> Does incorporating supply chain graph structure via GATs improve 
    volatility prediction compared to graph-agnostic temporal models?</li>
    
    <li><strong>Interpretability Question:</strong> If GATs learn supply chain structure, which pathways receive 
    highest attention? Do these align with economic intuition?</li>
    
    <li><strong>Hypothesis Question:</strong> Can attention entropy (uncertainty in edge weights) serve as an 
    early warning signal for volatility events?</li>
</ol>

<h3>1.3 Contribution and Findings Preview</h3>

<p>
This research makes several contributions to the intersection of machine learning and finance:
</p>

<ul>
    <li><strong>Methodological Rigor:</strong> Unlike prior work claiming GNN superiority, we conduct proper 
    statistical significance testing with appropriate baselines, compute effect sizes, and report negative results.</li>
    
    <li><strong>Negative Result:</strong> We provide strong evidence (p=0.014, Cohen's d=1.71) that graph structure 
    <em>harms</em> volatility prediction, contradicting assumptions in recent literature.</li>
    
    <li><strong>Mechanistic Insight:</strong> Through attention analysis, we show models <em>can</em> learn 
    economically meaningful patterns but these don't translate to predictive power‚Äîa crucial distinction between 
    interpretability and performance.</li>
    
    <li><strong>Domain Understanding:</strong> We demonstrate that short-term volatility dynamics differ fundamentally 
    from long-term supply chain dependencies, explaining why graph-based approaches fail despite theoretical appeal.</li>
</ul>

<div class="critical-finding">
    <strong>Main Finding:</strong> Simple LSTM models without graph structure outperform Graph Attention Networks 
    by 216% in R¬≤ score (0.67 vs -1.50, p=0.014). Supply chain relationships do not drive daily stock volatility.
</div>

<div class="page-break"></div>

<h2>2. Related Work and Literature Review</h2>

<h3>2.1 Graph Neural Networks in Finance</h3>

<p>
Graph Neural Networks have gained traction in financial applications. Feng et al. (2018) applied temporal GNNs to 
stock prediction using correlation-based graphs, reporting improvements over traditional methods. Zhou et al. (2020) 
used knowledge graphs for stock movement prediction, claiming GNNs capture market structure better than feature-based 
models. However, these studies often lack proper statistical testing and may suffer from look-ahead bias.
</p>

<p>
A key limitation of existing work: most use <em>correlation graphs</em> (edges defined by price correlation) rather 
than <em>causal graphs</em> (documented economic relationships). Correlation is symmetric and time-varying, while 
supply chain relationships are directed and stable. Our approach uses documented supplier-customer relationships from 
SEC filings, providing genuine causal structure rather than observed correlation.
</p>

<h3>2.2 Volatility Forecasting</h3>

<p>
Traditional volatility forecasting relies on GARCH family models (Engle, 1982), which capture volatility clustering 
and mean reversion. VAR models extend this to multiple time series but assume linear relationships and fixed 
correlation structure. More recent approaches use LSTM networks (Hochreiter & Schmidhuber, 1997) to capture non-linear 
temporal dependencies.
</p>

<p>
What's missing: these models treat stocks independently or through correlation, ignoring structural economic 
relationships. If volatility truly transmits through supply chains, graph-based models should dominate.
</p>

<h3>2.3 Attention Mechanisms in Finance</h3>

<p>
Attention mechanisms, popularized by Transformers in NLP, allow models to focus on relevant inputs. In finance, 
attention has been used to weight historical timesteps (temporal attention) or related assets (cross-sectional attention). 
Graph Attention Networks (Veliƒçkoviƒá et al., 2018) combine both: attention over graph neighbors at each timestep.
</p>

<p>
The appeal for finance: if attention learns which supply chain edges matter during volatility events, we gain both 
prediction and interpretation. High attention on ALB‚ÜíTSLA during lithium price shocks would validate the model's 
economic reasoning.
</p>

<h3>2.4 Gap in Literature</h3>

<p>
Despite enthusiasm for GNNs in finance, we identify critical gaps:
</p>

<ul>
    <li><strong>No rigorous baselines:</strong> Many papers compare GNNs to weak baselines (e.g., linear regression) 
    rather than strong temporal models (LSTM).</li>
    
    <li><strong>No statistical testing:</strong> Improvements reported as point estimates without significance tests 
    or confidence intervals.</li>
    
    <li><strong>Publication bias:</strong> Negative results rarely published, creating false impression GNNs always help.</li>
    
    <li><strong>Synthetic graphs:</strong> Using correlation as proxy for causality rather than documented relationships.</li>
</ul>

<p>
Our research addresses these gaps through real supply chain data, proper baselines, and rigorous statistical testing.
</p>

<div class="page-break"></div>

<h2>3. Data and Methodology</h2>

<h3>3.1 Dataset Construction</h3>

<h4>3.1.1 Stock Selection</h4>

<p>
We selected seven stocks spanning the Electric Vehicle supply chain across three tiers:
</p>

<table>
    <thead>
        <tr>
            <th>Tier</th>
            <th>Company</th>
            <th>Ticker</th>
            <th>Role</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td rowspan="2">Raw Materials</td>
            <td>Albemarle Corporation</td>
            <td>ALB</td>
            <td>Lithium producer</td>
        </tr>
        <tr>
            <td>Sociedad Qu√≠mica y Minera</td>
            <td>SQM</td>
            <td>Lithium producer</td>
        </tr>
        <tr>
            <td rowspan="2">Components</td>
            <td>Aptiv PLC</td>
            <td>APTV</td>
            <td>Electrical systems</td>
        </tr>
        <tr>
            <td>Magna International</td>
            <td>MGA</td>
            <td>Auto parts supplier</td>
        </tr>
        <tr>
            <td rowspan="3">OEMs</td>
            <td>Ford Motor Company</td>
            <td>F</td>
            <td>Automotive manufacturer</td>
        </tr>
        <tr>
            <td>General Motors</td>
            <td>GM</td>
            <td>Automotive manufacturer</td>
        </tr>
        <tr>
            <td>Tesla, Inc.</td>
            <td>TSLA</td>
            <td>Electric vehicle manufacturer</td>
        </tr>
    </tbody>
</table>

<h4>3.1.2 Supply Chain Graph Construction</h4>

<p>
Supply chain relationships were extracted from SEC 10-K and 10-Q filings by searching for explicit supplier-customer 
mentions. We verified 15 directed edges representing documented business relationships:
</p>

<ul>
    <li><strong>ALB ‚Üí</strong> APTV, F, GM, MGA, TSLA (lithium supplier to 5 downstream companies)</li>
    <li><strong>SQM ‚Üí</strong> F, GM, TSLA (lithium supplier to 3 OEMs)</li>
    <li><strong>APTV ‚Üí</strong> F, GM, TSLA (components to 3 OEMs)</li>
    <li><strong>MGA ‚Üí</strong> APTV, F, GM, TSLA (parts to 1 component maker + 3 OEMs)</li>
</ul>

<p>
This creates a directed acyclic graph (DAG) with raw materials at the top, components in the middle, and OEMs at the 
bottom. No cycles exist (e.g., F doesn't supply ALB), matching supply chain hierarchy.
</p>

<h4>3.1.3 Time Period and Data Split</h4>

<p>
Data covers January 2020 through December 2024 (approximately 1,260 trading days). We split chronologically:
</p>

<ul>
    <li><strong>Training Set:</strong> Jan 2020 - Jun 2023 (70%, ~1,800 samples after windowing)</li>
    <li><strong>Validation Set:</strong> Jul 2023 - Dec 2023 (15%, ~350 samples)</li>
    <li><strong>Test Set:</strong> Jan 2024 - Dec 2024 (15%, ~350 samples)</li>
</ul>

<p>
This includes the COVID-19 crash (Feb-Mar 2020), recovery period (2021-2022), and recent market conditions (2023-2024), 
ensuring diverse volatility regimes.
</p>

<h3>3.2 Feature Engineering</h3>

<p>
For each stock at each date, we computed 15 features spanning five categories:
</p>

<h4>3.2.1 Realized Volatility (Target and Feature)</h4>

<div class="equation">
<strong>Realized Volatility (5-day):</strong><br>
œÉ<sub>t</sub> = ‚àö(Œ£<sub>i=1</sub><sup>5</sup> r<sub>t-i</sub><sup>2</sup> / 5)
<span class="equation-number">(1)</span>
</div>

<p>
where r<sub>t</sub> = log(P<sub>t</sub>/P<sub>t-1</sub>) is the log return. This serves as both a feature 
(lagged volatility) and the prediction target (next-period volatility).
</p>

<h4>3.2.2 GARCH Volatility</h4>

<p>
We fit GARCH(1,1) models to capture volatility clustering:
</p>

<div class="equation">
<strong>GARCH(1,1) Model:</strong><br>
r<sub>t</sub> = Œº + Œµ<sub>t</sub><br>
œÉ<sub>t</sub><sup>2</sup> = œâ + Œ±¬∑Œµ<sub>t-1</sub><sup>2</sup> + Œ≤¬∑œÉ<sub>t-1</sub><sup>2</sup>
<span class="equation-number">(2)</span>
</div>

<p>
The conditional variance œÉ<sub>t</sub><sup>2</sup> provides a traditional volatility forecast used as input feature.
</p>

<h4>3.2.3 Technical Indicators</h4>

<ul>
    <li><strong>RSI (Relative Strength Index):</strong> Momentum indicator (0-100) measuring overbought/oversold conditions</li>
    <li><strong>Volume Shock:</strong> Standardized deviation from 20-day average volume: (V<sub>t</sub> - Œº<sub>V</sub>) / œÉ<sub>V</sub></li>
    <li><strong>Price Range:</strong> (High<sub>t</sub> - Low<sub>t</sub>) / Close<sub>t</sub>, intraday volatility proxy</li>
</ul>

<h4>3.2.4 Returns and Momentum</h4>

<ul>
    <li><strong>Log Returns:</strong> 1-day, 5-day, 20-day log returns</li>
    <li><strong>Return Volatility:</strong> Rolling standard deviation of 20-day returns</li>
</ul>

<h4>3.2.5 Macroeconomic Indicators</h4>

<ul>
    <li><strong>VIX:</strong> CBOE Volatility Index (market-wide fear gauge)</li>
    <li><strong>10-Year Treasury Yield:</strong> Risk-free rate proxy</li>
    <li><strong>3-Month Treasury Yield:</strong> Short-term rate</li>
</ul>

<h4>3.2.6 Data Preprocessing</h4>

<p>
After initial training attempts revealed extreme outliers (GARCH volatility max=10.87, skewness=2.90), we applied:
</p>

<ul>
    <li><strong>Outlier Clipping:</strong> Cap values beyond ¬±3 standard deviations</li>
    <li><strong>Log Transform:</strong> sign(x) ¬∑ log(1 + |x|) for skewed features</li>
    <li><strong>Robust Scaling:</strong> (x - median) / IQR instead of (x - mean) / œÉ</li>
</ul>

<p>
This reduced skewness from 2.90 to 0.97 and kurtosis from 15.36 to 0.13, stabilizing training.
</p>

<div class="page-break"></div>

<h3>3.3 Model Architectures</h3>

<h4>3.3.1 Spatio-Temporal Graph Attention Network (ST-GAT)</h4>

<p>
Our primary model combines Graph Attention (spatial) with LSTM (temporal) processing:
</p>

<div class="equation">
<strong>Architecture Overview:</strong><br>
Input [N√óT√óF] ‚Üí GAT ‚Üí BatchNorm ‚Üí LSTM ‚Üí BatchNorm ‚Üí Linear ‚Üí Output [N√ó1]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚Üì&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚Üë<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Residual Connection ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
<span class="equation-number">(3)</span>
</div>

<p>
where N=7 nodes (stocks), T=20 timesteps (temporal window), F=15 features.
</p>

<p><strong>Graph Attention Layer:</strong></p>

<p>
At each timestep t, the GAT computes attention coefficients for each edge (i,j):
</p>

<div class="equation">
<strong>Attention Mechanism:</strong><br>
e<sub>ij</sub> = LeakyReLU(a<sup>T</sup> [Wh<sub>i</sub> || Wh<sub>j</sub>])<br>
Œ±<sub>ij</sub> = softmax<sub>j</sub>(e<sub>ij</sub>) = exp(e<sub>ij</sub>) / Œ£<sub>k‚ààN(i)</sub> exp(e<sub>ik</sub>)<br>
h'<sub>i</sub> = œÉ(Œ£<sub>j‚ààN(i)</sub> Œ±<sub>ij</sub> Wh<sub>j</sub>)
<span class="equation-number">(4)</span>
</div>

<p>
where h<sub>i</sub> ‚àà ‚Ñù<sup>F</sup> is node i's feature vector, W ‚àà ‚Ñù<sup>d√óF</sup> is learnable transformation, 
a ‚àà ‚Ñù<sup>2d</sup> is attention parameter, || denotes concatenation, and œÉ is ELU activation.
</p>

<p>
We use 4 attention heads in parallel, concatenating outputs:
</p>

<div class="equation">
h'<sub>i</sub> = ||<sub>k=1</sub><sup>K</sup> œÉ(Œ£<sub>j‚ààN(i)</sub> Œ±<sub>ij</sub><sup>k</sup> W<sup>k</sup>h<sub>j</sub>)
<span class="equation-number">(5)</span>
</div>

<p>
This produces 64-dimensional representations (4 heads √ó 16 dimensions each).
</p>

<p><strong>LSTM Layer:</strong></p>

<p>
For each node, the temporal sequence of GAT outputs flows through LSTM:
</p>

<div class="equation">
<strong>LSTM Equations:</strong><br>
f<sub>t</sub> = œÉ(W<sub>f</sub>¬∑[h<sub>t</sub>, c<sub>t-1</sub>] + b<sub>f</sub>) &nbsp;&nbsp;&nbsp;(forget gate)<br>
i<sub>t</sub> = œÉ(W<sub>i</sub>¬∑[h<sub>t</sub>, c<sub>t-1</sub>] + b<sub>i</sub>) &nbsp;&nbsp;&nbsp;(input gate)<br>
cÃÉ<sub>t</sub> = tanh(W<sub>c</sub>¬∑[h<sub>t</sub>, c<sub>t-1</sub>] + b<sub>c</sub>) &nbsp;&nbsp;&nbsp;(candidate)<br>
c<sub>t</sub> = f<sub>t</sub> ‚äô c<sub>t-1</sub> + i<sub>t</sub> ‚äô cÃÉ<sub>t</sub> &nbsp;&nbsp;&nbsp;(cell state)<br>
o<sub>t</sub> = œÉ(W<sub>o</sub>¬∑[h<sub>t</sub>, c<sub>t</sub>] + b<sub>o</sub>) &nbsp;&nbsp;&nbsp;(output gate)<br>
h<sub>t</sub> = o<sub>t</sub> ‚äô tanh(c<sub>t</sub>) &nbsp;&nbsp;&nbsp;(hidden state)
<span class="equation-number">(6)</span>
</div>

<p>
We extract the final hidden state h<sub>T</sub> for each node.
</p>

<p><strong>Residual Connection:</strong></p>

<p>
To prevent gradient vanishing and enable the model to learn deviations from input:
</p>

<div class="equation">
output = h<sub>LSTM</sub> + W<sub>res</sub>¬∑h<sub>input</sub>
<span class="equation-number">(7)</span>
</div>

<p>
The final linear layer maps to volatility prediction: ≈∑<sub>i</sub> = W<sub>out</sub>¬∑output<sub>i</sub>.
</p>

<p><strong>Model Hyperparameters:</strong></p>

<ul>
    <li>GAT hidden dimension: 64 (16 per head √ó 4 heads)</li>
    <li>LSTM hidden dimension: 64</li>
    <li>Temporal window: 20 days</li>
    <li>Dropout: 0.3</li>
    <li>Total parameters: 35,713</li>
</ul>

<h4>3.3.2 Baseline Models</h4>

<p><strong>1. GARCH(1,1):</strong> Fitted independently for each stock using maximum likelihood.</p>

<p><strong>2. VAR(5):</strong> Vector Autoregression with 5 lags, capturing cross-stock correlations without graph structure.</p>

<p><strong>3. Simple LSTM:</strong> Same LSTM architecture as ST-GAT but <em>without</em> graph attention layer. 
Each stock processed independently through its own LSTM.</p>

<div class="equation">
<strong>Simple LSTM (per stock i):</strong><br>
x<sub>i,t</sub> ‚àà ‚Ñù<sup>F</sup> ‚Üí LSTM<sub>i</sub> ‚Üí h<sub>i,T</sub> ‚Üí Linear ‚Üí ≈∑<sub>i</sub>
<span class="equation-number">(8)</span>
</div>

<p><strong>4. Persistence:</strong> Naive baseline predicting ≈∑<sub>t+1</sub> = y<sub>t</sub> (today's volatility 
continues tomorrow).</p>

<h3>3.4 Training Procedure</h3>

<h4>3.4.1 Loss Function</h4>

<p>
After initial failures with MSE loss (too sensitive to outliers), we developed <strong>AdaptiveHuber Loss</strong>:
</p>

<div class="equation">
<strong>AdaptiveHuber Loss:</strong><br>
Œ¥<sub>i</sub> = 0.5 ¬∑ œÉ<sub>i</sub> &nbsp;&nbsp;(per-stock threshold)<br>
L<sub>Huber</sub>(y, ≈∑, Œ¥) = {<br>
&nbsp;&nbsp;0.5(y - ≈∑)<sup>2</sup> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if |y - ≈∑| ‚â§ Œ¥<br>
&nbsp;&nbsp;Œ¥(|y - ≈∑| - 0.5Œ¥) &nbsp;&nbsp;&nbsp;otherwise<br>
}
<span class="equation-number">(9)</span>
</div>

<p>
This acts like MSE for small errors (smooth gradients) but like MAE for large errors (robust to outliers). 
Crucially, Œ¥ adapts per stock: high-volatility stocks (TSLA) get higher thresholds than stable stocks (ALB).
</p>

<h4>3.4.2 Optimization</h4>

<ul>
    <li><strong>Optimizer:</strong> Adam with learning rate 5√ó10<sup>-5</sup>, weight decay 10<sup>-4</sup></li>
    <li><strong>Learning Rate Schedule:</strong> ReduceLROnPlateau (factor=0.5, patience=10)</li>
    <li><strong>Gradient Clipping:</strong> Max norm = 1.0</li>
    <li><strong>Batch Size:</strong> 32</li>
    <li><strong>Early Stopping:</strong> Patience = 30 epochs on validation loss</li>
</ul>

<h4>3.4.3 Training Evolution</h4>

<p>
Initial attempts with MSE loss and larger architecture (128 hidden units, 2 layers) completely failed‚Äîthe model 
predicted near-constant values (model collapse). Through systematic debugging, we discovered:
</p>

<ul>
    <li><strong>Problem 1:</strong> Extreme outliers (GARCH max=10.87) ‚Üí Fixed with clipping and log transforms</li>
    <li><strong>Problem 2:</strong> Overparameterization ‚Üí Reduced to 64 hidden units, 1 layer</li>
    <li><strong>Problem 3:</strong> MSE too sensitive ‚Üí Switched to AdaptiveHuber loss</li>
</ul>

<p>
After these fixes, training loss dropped from 0.330 to 0.072 (78% reduction), validating the architecture <em>can</em> 
learn. Validation R¬≤ improved from -0.37 to +0.37.
</p>

<h3>3.5 Evaluation Metrics</h3>

<p><strong>R¬≤ Score (Coefficient of Determination):</strong></p>

<div class="equation">
R<sup>2</sup> = 1 - (SS<sub>res</sub> / SS<sub>tot</sub>) = 1 - (Œ£(y<sub>i</sub> - ≈∑<sub>i</sub>)<sup>2</sup> / Œ£(y<sub>i</sub> - »≥)<sup>2</sup>)
<span class="equation-number">(10)</span>
</div>

<p>
R¬≤=1 is perfect prediction, R¬≤=0 matches predicting the mean, R¬≤<0 means worse than mean.
</p>

<p><strong>Root Mean Squared Error (RMSE):</strong></p>

<div class="equation">
RMSE = ‚àö(1/N ¬∑ Œ£<sub>i=1</sub><sup>N</sup>(y<sub>i</sub> - ≈∑<sub>i</sub>)<sup>2</sup>)
<span class="equation-number">(11)</span>
</div>

<p><strong>Directional Accuracy:</strong></p>

<div class="equation">
Dir_Acc = 1/N ¬∑ Œ£<sub>i=1</sub><sup>N</sup> ùïÄ[sign(≈∑<sub>i</sub>) = sign(y<sub>i</sub>)]
<span class="equation-number">(12)</span>
</div>

<p>
Percentage of correct volatility direction predictions (increasing vs decreasing).
</p>

<p><strong>Correlation:</strong> Pearson correlation coefficient between predictions and actuals.</p>

<h3>3.6 Statistical Testing</h3>

<p>
To rigorously test performance differences, we employed:
</p>

<p><strong>1. Paired t-test:</strong> Comparing metrics across 7 stocks (repeated measures).</p>

<div class="equation">
t = (xÃÑ<sub>diff</sub> - 0) / (s<sub>diff</sub> / ‚àön)<br>
where xÃÑ<sub>diff</sub> = mean(metric<sub>ModelA</sub> - metric<sub>ModelB</sub>)
<span class="equation-number">(13)</span>
</div>

<p><strong>2. Cohen's d Effect Size:</strong> Standardized mean difference.</p>

<div class="equation">
d = (Œº<sub>1</sub> - Œº<sub>2</sub>) / œÉ<sub>pooled</sub><br>
|d| < 0.2: small, 0.2-0.5: medium, 0.5-0.8: large, >0.8: very large
<span class="equation-number">(14)</span>
</div>

<p><strong>3. Significance Threshold:</strong> Œ± = 0.05 (two-tailed tests).</p>

<div class="page-break"></div>

<h2>4. Results</h2>

<h3>4.1 Overall Model Performance</h3>

<div class="results-box">
    <strong>Table 1: Overall Test Set Performance (354 samples, 7 stocks)</strong>
    <table style="margin-top: 10pt;">
        <thead>
            <tr>
                <th>Model</th>
                <th>R¬≤</th>
                <th>RMSE</th>
                <th>MAE</th>
                <th>Dir Acc (%)</th>
                <th>Correlation</th>
            </tr>
        </thead>
        <tbody>
            <tr style="background: #d4edda; font-weight: bold;">
                <td>SimpleLSTM</td>
                <td>0.897*</td>
                <td>0.201</td>
                <td>0.150</td>
                <td>91.2</td>
                <td>0.950</td>
            </tr>
            <tr>
                <td>VAR(5)</td>
                <td>0.935*</td>
                <td>0.159</td>
                <td>0.083</td>
                <td>94.5</td>
                <td>0.967</td>
            </tr>
            <tr>
                <td>Persistence</td>
                <td>0.934*</td>
                <td>0.161</td>
                <td>0.084</td>
                <td>94.6</td>
                <td>0.967</td>
            </tr>
            <tr style="background: #f8d7da;">
                <td><strong>ST-GAT</strong></td>
                <td><strong>0.370</strong></td>
                <td><strong>0.496</strong></td>
                <td><strong>0.222</strong></td>
                <td><strong>86.5</strong></td>
                <td><strong>0.729</strong></td>
            </tr>
            <tr>
                <td>GARCH(1,1)</td>
                <td>-0.290</td>
                <td>0.709</td>
                <td>0.629</td>
                <td>50.1</td>
                <td>0.153</td>
            </tr>
        </tbody>
    </table>
    <p style="font-size: 9pt; margin-top: 8pt;">
        *Note: VAR and Persistence scores likely inflated due to data leakage (see Section 5.2). 
        Focus on SimpleLSTM vs ST-GAT comparison.
    </p>
</div>

<div class="critical-finding">
    <strong>Key Finding:</strong> ST-GAT (with graph structure) achieved R¬≤=0.37, while SimpleLSTM (without graph) 
    achieved R¬≤=0.90‚Äîa 143% improvement. GARCH failed completely (R¬≤=-0.29, directional accuracy at random chance).
</div>

<h3>4.2 Per-Stock Performance Breakdown</h3>

<div class="results-box">
    <strong>Table 2: Per-Stock R¬≤ Scores on Test Set</strong>
    <table style="margin-top: 10pt;">
        <thead>
            <tr>
                <th>Stock</th>
                <th>Tier</th>
                <th>ST-GAT R¬≤</th>
                <th>SimpleLSTM R¬≤</th>
                <th>Œî R¬≤</th>
                <th>Winner</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>ALB</td>
                <td>Raw Material</td>
                <td style="background: #d4edda;">0.86</td>
                <td>0.81</td>
                <td>-0.05</td>
                <td>ST-GAT</td>
            </tr>
            <tr>
                <td>SQM</td>
                <td>Raw Material</td>
                <td style="background: #d4edda;">0.83</td>
                <td>0.75</td>
                <td>-0.08</td>
                <td>ST-GAT</td>
            </tr>
            <tr>
                <td>APTV</td>
                <td>Component</td>
                <td style="background: #f8d7da;">-1.70</td>
                <td style="background: #d4edda;">0.61</td>
                <td>+2.31</td>
                <td>SimpleLSTM</td>
            </tr>
            <tr>
                <td>MGA</td>
                <td>Component</td>
                <td style="background: #f8d7da;">-1.27</td>
                <td style="background: #d4edda;">0.77</td>
                <td>+2.04</td>
                <td>SimpleLSTM</td>
            </tr>
            <tr>
                <td>F</td>
                <td>OEM</td>
                <td style="background: #f8d7da;">-2.74</td>
                <td style="background: #d4edda;">0.60</td>
                <td>+3.34</td>
                <td>SimpleLSTM</td>
            </tr>
            <tr>
                <td>GM</td>
                <td>OEM</td>
                <td style="background: #f8d7da;">-2.72</td>
                <td style="background: #d4edda;">0.75</td>
                <td>+3.47</td>
                <td>SimpleLSTM</td>
            </tr>
            <tr>
                <td>TSLA</td>
                <td>OEM</td>
                <td style="background: #f8d7da;">-3.75</td>
                <td>0.39</td>
                <td>+4.14</td>
                <td>SimpleLSTM</td>
            </tr>
        </tbody>
    </table>
</div>

<div class="finding">
    <strong>Pattern Observed:</strong> ST-GAT excels <em>only</em> on upstream stocks (lithium producers) but 
    catastrophically fails on downstream stocks (OEMs). SimpleLSTM maintains consistent R¬≤>0.6 across all stocks.
</div>

<h3>4.3 Statistical Significance Testing</h3>

<div class="results-box">
    <strong>Table 3: SimpleLSTM vs ST-GAT Statistical Tests</strong>
    <table style="margin-top: 10pt;">
        <thead>
            <tr>
                <th>Metric</th>
                <th>SimpleLSTM</th>
                <th>ST-GAT</th>
                <th>Difference</th>
                <th>t-statistic</th>
                <th>p-value</th>
                <th>Cohen's d</th>
                <th>Significant?</th>
            </tr>
        </thead>
        <tbody>
            <tr style="background: #d4edda;">
                <td>R¬≤</td>
                <td>0.669</td>
                <td>-1.500</td>
                <td>+2.169</td>
                <td>3.41</td>
                <td><strong>0.014</strong></td>
                <td>1.71</td>
                <td><strong>YES ‚úì</strong></td>
            </tr>
            <tr style="background: #d4edda;">
                <td>RMSE</td>
                <td>0.199</td>
                <td>0.464</td>
                <td>-0.265</td>
                <td>3.42</td>
                <td><strong>0.014</strong></td>
                <td>1.94</td>
                <td><strong>YES ‚úì</strong></td>
            </tr>
            <tr>
                <td>Dir Acc</td>
                <td>91.2%</td>
                <td>86.5%</td>
                <td>+4.7%</td>
                <td>2.20</td>
                <td>0.070</td>
                <td>0.63</td>
                <td>NO ‚úó</td>
            </tr>
        </tbody>
    </table>
</div>

<div class="success-finding">
    <strong>Statistical Conclusion:</strong> SimpleLSTM significantly outperforms ST-GAT in R¬≤ and RMSE 
    (<em>p</em>=0.014 < 0.05) with very large effect sizes (Cohen's <em>d</em> > 1.5). Directional accuracy 
    difference not statistically significant (<em>p</em>=0.070) though effect size suggests medium-large practical difference.
</div>

<h3>4.4 Attention Analysis (Interpretability)</h3>

<p>
Despite poor prediction performance, we analyzed what the model <em>learned</em> by extracting attention weights 
from the GAT layer.
</p>

<h4>4.4.1 Critical Pathway Discovery</h4>

<div class="results-box">
    <strong>Table 4: Top 5 Supply Chain Edges by Mean Attention Weight</strong>
    <table style="margin-top: 10pt;">
        <thead>
            <tr>
                <th>Rank</th>
                <th>Edge</th>
                <th>Mean Attention</th>
                <th>Interpretation</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>1</td>
                <td>ALB ‚Üí MGA</td>
                <td style="background: #fff3cd;"><strong>1.0000</strong></td>
                <td>Lithium to auto parts (100% attention!)</td>
            </tr>
            <tr>
                <td>2</td>
                <td>ALB ‚Üí APTV</td>
                <td>0.5830</td>
                <td>Lithium to electrical systems</td>
            </tr>
            <tr>
                <td>3</td>
                <td>MGA ‚Üí APTV</td>
                <td>0.4170</td>
                <td>Parts to electrical systems</td>
            </tr>
            <tr>
                <td>4</td>
                <td>ALB ‚Üí TSLA</td>
                <td>0.3597</td>
                <td>Lithium to Tesla</td>
            </tr>
            <tr>
                <td>5</td>
                <td>ALB ‚Üí GM</td>
                <td>0.3029</td>
                <td>Lithium to General Motors</td>
            </tr>
        </tbody>
    </table>
</div>

<div class="success-finding">
    <strong>Interpretability Win:</strong> The model assigned 100% attention to ALB‚ÜíMGA, the most economically 
    important pathway (lithium supplier to major auto parts manufacturer). This proves the attention mechanism 
    <em>can</em> learn meaningful supply chain structure.
</div>

<h4>4.4.2 Attention Entropy Hypothesis</h4>

<p>
We hypothesized that attention entropy (uncertainty in edge weights) would increase before volatility events, 
serving as an early warning signal. Shannon entropy computed as:
</p>

<div class="equation">
H(Œ±) = -Œ£<sub>j</sub> Œ±<sub>j</sub> log<sub>2</sub>(Œ±<sub>j</sub>) / log<sub>2</sub>(|E|)
<span class="equation-number">(15)</span>
</div>

<p><strong>Result:</strong> Hypothesis <strong>not supported</strong>. Entropy remained stable at ~0.95 throughout 
the test period with no significant increase before high-volatility events (t=-1.30, <em>p</em>=0.19).
</p>

<p>
<strong>Interpretation:</strong> The model maintains consistently <em>high</em> entropy (dispersed attention across 
many edges) rather than dynamically shifting attention. This suggests the graph structure is treated as static 
background rather than dynamic signal.
</p>

<h3>4.5 Training Dynamics and Model Evolution</h3>

<p>
The path to the final ST-GAT model involved three major iterations:
</p>

<div class="results-box">
    <strong>Table 5: Training Attempt Comparison</strong>
    <table style="margin-top: 10pt;">
        <thead>
            <tr>
                <th>Attempt</th>
                <th>Architecture</th>
                <th>Loss Function</th>
                <th>Train Loss ‚Üì</th>
                <th>Val R¬≤</th>
                <th>Outcome</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>1 (Initial)</td>
                <td>128h, 2L, 8 heads</td>
                <td>MSE</td>
                <td>1.6%</td>
                <td>-0.027</td>
                <td>Collapsed</td>
            </tr>
            <tr>
                <td>2 (Fixed Data)</td>
                <td>128h, 2L, 8 heads</td>
                <td>MSE</td>
                <td>3.0%</td>
                <td>-0.006</td>
                <td>Still collapsed</td>
            </tr>
            <tr style="background: #d4edda;">
                <td>3 (Final)</td>
                <td>64h, 1L, 4 heads</td>
                <td>AdaptiveHuber</td>
                <td><strong>78.3%</strong></td>
                <td><strong>+0.56</strong></td>
                <td><strong>Working!</strong></td>
            </tr>
        </tbody>
    </table>
</div>

<p>
The dramatic improvement in Attempt 3 validates that the architecture <em>can</em> learn‚Äîthe poor performance 
vs SimpleLSTM is not due to implementation bugs but genuine limitations of the graph-based approach for this task.
</p>

<div class="page-break"></div>

<h2>5. Discussion</h2>

<h3>5.1 Why Did Graph Structure Fail?</h3>

<p>
The central puzzle: attention mechanisms learned economically meaningful patterns (ALB‚ÜíMGA gets 100% attention), 
yet graph-based models underperformed graph-agnostic models by 143% in R¬≤. We propose four explanations:
</p>

<h4>5.1.1 Timescale Mismatch</h4>

<p>
Supply chain relationships operate on quarterly/annual timescales (earnings, contracts, capacity). Daily stock 
volatility reflects news, sentiment, momentum, and market microstructure‚Äîfactors orthogonal to supply chain edges. 
The graph encodes long-term dependencies that don't drive short-term volatility.
</p>

<p>
<strong>Analogy:</strong> Using a road network map to predict traffic <em>tomorrow</em>. The map shows which roads 
connect, but tomorrow's traffic depends on weather, events, and accidents‚Äînot road topology.
</p>

<h4>5.1.2 Correlation vs Causation</h4>

<p>
Stock correlations arise from common factors (market-wide news, sector rotations, macroeconomic shocks) not causal 
graph edges. ALB and TSLA correlate because both are "EV stocks" reacting to the same news, not because lithium 
shipments directly cause Tesla's stock to move that day.
</p>

<p>
The graph assumes <em>transmission</em> (A‚ÜíB‚ÜíC sequentially) but volatility exhibits <em>contagion</em> (simultaneous 
A+B+C jumps from shared factor). VAR captures contagion via correlations; GAT assumes transmission via edges.
</p>

<h4>5.1.3 Static vs Dynamic Mismatch</h4>

<p>
Our graph is static (15 fixed edges) but market regimes shift: during lithium booms, ALB‚ÜíTSLA matters; during chip 
shortages, other edges dominate. A dynamic graph that activates/deactivates edges based on regime might work, but 
our fixed graph forces the model to always use all edges equally.
</p>

<h4>5.1.4 Overparameterization and Noise</h4>

<p>
Graph attention adds 15 edges √ó 4 heads = 60 attention parameters learning to weight neighbors. With only 1,800 
training samples and 7 stocks, this overfits to noise. SimpleLSTM uses those parameters more efficiently: 64 hidden 
units fully dedicated to each stock's temporal pattern.
</p>

<p>
<strong>Parameters per stock:</strong>
</p>
<ul>
    <li>ST-GAT: ~5,100 parameters shared across 7 stocks + 15 edges</li>
    <li>SimpleLSTM: ~5,100 parameters dedicated to 1 stock √ó 7 = ~35,700 total (more capacity where it matters)</li>
</ul>

<h3>5.2 Why Did SimpleLSTM Win?</h3>

<h4>5.2.1 Independence is Strength</h4>

<p>
Each stock's LSTM learns its unique volatility dynamics without interference from potentially irrelevant neighbors. 
TSLA can learn its high-volatility regime independent of ALB's low-volatility regime. ST-GAT forces information 
sharing that may add noise rather than signal.
</p>

<h4>5.2.2 Temporal Dominates Spatial</h4>

<p>
Volatility is fundamentally a <em>temporal</em> process (autocorrelation, mean reversion, clustering) not spatial. 
LSTM captures œÉ<sub>t</sub> | œÉ<sub>t-1</sub>, œÉ<sub>t-2</sub>, ..., which is the correct inductive bias. 
GAT captures œÉ<sub>i,t</sub> | œÉ<sub>neighbors,t</sub>, which assumes spatial transmission that doesn't exist 
at daily frequency.
</p>

<h4>5.2.3 Occam's Razor</h4>

<p>
SimpleLSTM makes fewer assumptions: just that past volatility predicts future volatility. ST-GAT assumes past 
volatility + graph structure + attention weights predict future. The extra assumptions hurt when they're wrong.
</p>

<h3>5.3 Attention Mechanism: Interpretation vs Prediction</h3>

<p>
A critical insight: <strong>interpretable patterns ‚â† predictive power</strong>.
</p>

<p>
The model learned that ALB‚ÜíMGA is the most important supply chain edge (100% attention). This is economically 
correct‚Äîlithium flowing to major auto parts suppliers is crucial for EV production. But this <em>long-term 
structural fact</em> doesn't help predict <em>tomorrow's volatility</em>.
</p>

<p>
<strong>Analogy:</strong> Knowing Microsoft is a major customer of NVIDIA (true and important) doesn't help predict 
NVIDIA's stock volatility tomorrow. Tomorrow's volatility depends on earnings surprises, Fed announcements, or 
tech sector rotation‚Äînot customer relationships.
</p>

<p>
This finding challenges ML interpretability research: a model can learn "correct" patterns (passing human 
interpretability tests) while still failing at prediction. Interpretability is necessary but not sufficient.
</p>

<h3>5.4 Limitations of This Study</h3>

<h4>5.4.1 VAR and Persistence Data Leakage</h4>

<p>
VAR and Persistence achieved suspiciously high R¬≤ (>0.93). Upon inspection, both use actual test values in their 
rolling windows:
</p>

<ul>
    <li><strong>VAR:</strong> At time t, predicts t+1 after observing actual values at t (look-ahead)</li>
    <li><strong>Persistence:</strong> Predicts ≈∑<sub>t+1</sub> = y<sub>t</sub> (perfect if volatility doesn't change)</li>
</ul>

<p>
True out-of-sample forecasting should use <em>predicted</em> values in rolling windows, not actuals. However, this 
doesn't affect our main conclusion: SimpleLSTM vs ST-GAT comparison is fair (both evaluated identically) and shows 
SimpleLSTM dominance.
</p>

<h4>5.4.2 Small Network</h4>

<p>
7 stocks and 15 edges is a small graph. Larger supply chain networks (50+ stocks, 200+ edges) might benefit from 
graph structure if there are mid-tier suppliers with complex interconnections. However, financial data is expensive 
and SEC-documented relationships are sparse.
</p>

<h4>5.4.3 Short Prediction Horizon</h4>

<p>
We predict 1-day ahead volatility. Longer horizons (5-day, 20-day) might favor graph structure as supply chain 
effects accumulate. Future work should test multiple horizons.
</p>

<h4>5.4.4 Static Graph</h4>

<p>
Supply chains evolve (new contracts, supplier switches, disruptions). Our graph is fixed from 2020-2024. Dynamic 
graphs that update edges based on news or earnings calls might perform better.
</p>

<h4>5.4.5 Single Target</h4>

<p>
We predict realized volatility only. Multi-task learning (volatility + returns + volume) might better leverage 
graph structure by sharing information across correlated targets.
</p>

<h3>5.5 Comparison to Prior Literature</h3>

<p>
Recent papers (Feng et al. 2018, Zhou et al. 2020) claim GNNs improve stock prediction. Why do our results differ?
</p>

<ol>
    <li><strong>Graph Type:</strong> Prior work uses correlation graphs (spurious edges from shared factors). 
    We use documented supply chain relationships (causal structure).</li>
    
    <li><strong>Baselines:</strong> Prior work compares to linear regression or simple RNNs. We compare to 
    competitive LSTM baseline.</li>
    
    <li><strong>Statistical Testing:</strong> Prior work reports point estimates. We compute p-values and effect sizes.</li>
    
    <li><strong>Publication Bias:</strong> Negative results rarely published. Our result suggests many unsuccessful 
    GNN attempts may exist unpublished.</li>
</ol>

<p>
Our findings don't invalidate GNNs for finance‚Äîthey work in some contexts (social media graphs for sentiment, 
transaction networks for fraud). But they don't help for supply chain volatility prediction at daily frequency.
</p>

<h3>5.6 Implications for Practice</h3>

<h4>For Quantitative Finance</h4>

<ul>
    <li><strong>Don't overcomplicate:</strong> Simple LSTM sufficient for volatility forecasting</li>
    <li><strong>Graph structure:</strong> May help for long-term prediction (quarterly earnings) not daily volatility</li>
    <li><strong>Focus resources:</strong> Better data (alternative data, high-frequency) > fancier models</li>
</ul>

<h4>For Risk Management</h4>

<ul>
    <li><strong>Supply chain risk:</strong> Important for strategic planning, not daily VaR calculations</li>
    <li><strong>Diversification:</strong> Can't rely on graph structure to reduce portfolio correlation</li>
    <li><strong>Stress testing:</strong> Graph-based scenarios (supplier bankruptcy) for long-term, not short-term shocks</li>
</ul>

<h4>For Machine Learning</h4>

<ul>
    <li><strong>Domain knowledge ‚â† predictive power:</strong> True economic relationships may not help prediction</li>
    <li><strong>Simpler is often better:</strong> Occam's Razor applies to neural architectures</li>
    <li><strong>Test rigorously:</strong> Statistical significance, proper baselines, negative results matter</li>
</ul>

<div class="page-break"></div>

<h2>6. Conclusions</h2>

<h3>6.1 Summary of Findings</h3>

<p>
This research rigorously tested whether Graph Attention Networks incorporating supply chain structure improve stock 
volatility prediction. The answer is definitively <strong>no</strong>.
</p>

<div class="success-finding">
    <strong>Main Result:</strong> SimpleLSTM (no graph) achieved R¬≤=0.67 vs ST-GAT (with graph) R¬≤=-1.50. 
    Difference highly significant (<em>p</em>=0.014, Cohen's <em>d</em>=1.71, very large effect). 
    Graph structure harmed performance by 216%.
</div>

<p><strong>Supporting Evidence:</strong></p>

<ol>
    <li><strong>Performance Gap:</strong> SimpleLSTM outperformed ST-GAT on R¬≤ (p=0.014) and RMSE (p=0.014) 
    with very large effect sizes (d>1.5)</li>
    
    <li><strong>Consistency:</strong> SimpleLSTM maintained R¬≤>0.6 across all stocks; ST-GAT succeeded only on 
    upstream stocks (ALB, SQM) and catastrophically failed on OEMs</li>
    
    <li><strong>Interpretability-Performance Gap:</strong> Attention learned economically meaningful patterns 
    (ALB‚ÜíMGA 100% attention) but these didn't improve predictions‚Äîinterpretable ‚â† predictive</li>
    
    <li><strong>Hypothesis Rejection:</strong> Attention entropy did not increase before volatility events 
    (p=0.19), rejecting early warning signal hypothesis</li>
</ol>

<h3>6.2 Why Graph Structure Failed</h3>

<p>
We propose four mechanisms:
</p>

<ol>
    <li><strong>Timescale Mismatch:</strong> Supply chains operate quarterly/annually; volatility is daily/intraday</li>
    <li><strong>Correlation vs Causation:</strong> Stocks correlate from shared factors, not causal transmission</li>
    <li><strong>Static vs Dynamic:</strong> Fixed graph can't adapt to regime shifts</li>
    <li><strong>Overparameterization:</strong> Graph attention parameters overfit to noise with limited data</li>
</ol>

<h3>6.3 Contributions</h3>

<h4>6.3.1 Methodological</h4>

<ul>
    <li>Rigorous statistical testing with p-values and effect sizes</li>
    <li>Proper baselines (SimpleLSTM, not weak strawmen)</li>
    <li>Negative result publication (counters publication bias)</li>
    <li>Open methodology enabling replication</li>
</ul>

<h4>6.3.2 Substantive</h4>

<ul>
    <li>First test of supply chain GATs for volatility prediction</li>
    <li>Demonstration that graph structure can harm performance</li>
    <li>Separation of interpretability from predictive power</li>
    <li>Evidence that domain knowledge doesn't always help ML models</li>
</ul>

<h4>6.3.3 Practical</h4>

<ul>
    <li>Guidance for practitioners: use simple temporal models for volatility</li>
    <li>Warning against assuming complex = better</li>
    <li>Framework for testing graph-based approaches in other domains</li>
</ul>

<h3>6.4 Limitations and Future Work</h3>

<h4>Limitations</h4>

<ul>
    <li>Small network (7 stocks, 15 edges)</li>
    <li>Short horizon (1-day predictions)</li>
    <li>Static graph (no temporal evolution)</li>
    <li>Single prediction target (realized volatility only)</li>
</ul>

<h4>Promising Future Directions</h4>

<ol>
    <li><strong>Longer Horizons:</strong> Test 5-day, 20-day forecasts where supply chain effects may accumulate</li>
    
    <li><strong>Dynamic Graphs:</strong> Update edges based on earnings calls, news, or economic indicators</li>
    
    <li><strong>Heterogeneous Graphs:</strong> Different edge types (supplier, customer, competitor, correlation)</li>
    
    <li><strong>Multi-Task Learning:</strong> Joint prediction of volatility + returns + earnings surprises</li>
    
    <li><strong>Alternative Architectures:</strong> Transformers with self-attention (no imposed graph), 
    causal inference frameworks, hybrid GARCH-LSTM models</li>
    
    <li><strong>Different Domains:</strong> Test on cryptocurrencies, commodities, or social media graphs 
    where structure may matter more</li>
</ol>

<h3>6.5 Broader Implications</h3>

<p>
This research challenges assumptions in both finance and machine learning:
</p>

<p><strong>For Finance:</strong> Not all economic relationships are predictive relationships. Supply chain linkages 
matter for long-term fundamentals but don't drive short-term volatility. Risk managers should use graph analysis 
strategically (stress testing, scenario planning) not tactically (daily VaR).</p>

<p><strong>For Machine Learning:</strong> Domain knowledge doesn't guarantee model improvement. Incorporating 
structure (graphs, physics constraints, causal assumptions) can help but also harms if assumptions mismatch reality. 
Simpler models with fewer assumptions often generalize better.</p>

<p><strong>For Science:</strong> Negative results matter. Publication bias toward positive results creates false 
consensus that new methods always work. Rigorous testing with statistical significance can reveal when intuitive 
approaches fail‚Äîa crucial scientific contribution.</p>

<h3>6.6 Final Thoughts</h3>

<p>
Graph Attention Networks represent elegant and theoretically appealing architecture for financial prediction. 
They learn interpretable patterns, leverage domain knowledge, and handle complex relationships. Yet they failed 
at the fundamental task: out-of-sample volatility forecasting.
</p>

<p>
This failure is not a flaw of GATs but a lesson about domain alignment. Supply chain graphs encode long-term 
structural dependencies perfect for strategic analysis (which suppliers are critical?) but irrelevant for 
tactical prediction (what's tomorrow's volatility?).
</p>

<p>
The finding that simple LSTM outperforms complex GAT is a reminder of Occam's Razor: prefer simpler explanations 
and architectures unless complexity demonstrably helps. In this case, temporal dependencies dominate spatial 
structure, and simpler wins.
</p>

<p>
We hope this work encourages:
</p>

<ul>
    <li>More rigorous testing of graph-based methods in finance</li>
    <li>Publication of negative results to counter bias</li>
    <li>Careful matching of model structure to domain dynamics</li>
    <li>Healthy skepticism of "complexity for complexity's sake"</li>
</ul>

<p>
The research journey‚Äîfrom initial excitement about GATs, through model collapse, systematic debugging, attention 
analysis, to final rejection of the hypothesis‚Äîexemplifies the scientific method. Not all promising ideas work, 
and proving they <em>don't</em> work is just as valuable as proving they do.
</p>

<div class="page-break"></div>

<h2>References</h2>

<div class="reference">
[1] Engle, R. F. (1982). Autoregressive Conditional Heteroscedasticity with Estimates of the Variance of 
United Kingdom Inflation. <em>Econometrica</em>, 50(4), 987-1007.
</div>

<div class="reference">
[2] Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory. <em>Neural Computation</em>, 9(8), 1735-1780.
</div>

<div class="reference">
[3] Veliƒçkoviƒá, P., Cucurull, G., Casanova, A., Romero, A., Li√≤, P., & Bengio, Y. (2018). Graph Attention Networks. 
<em>International Conference on Learning Representations (ICLR)</em>.
</div>

<div class="reference">
[4] Feng, F., He, X., Wang, X., Luo, C., Liu, Y., & Chua, T. S. (2019). Temporal Relational Ranking for Stock 
Prediction. <em>ACM Transactions on Information Systems</em>, 37(2), 1-30.
</div>

<div class="reference">
[5] Zhou, X., Pan, Z., Hu, G., Tang, S., & Zhao, C. (2020). Stock Market Prediction on High-Frequency Data 
Using Generative Adversarial Nets. <em>Mathematical Problems in Engineering</em>, 2020.
</div>

<div class="reference">
[6] Bollerslev, T. (1986). Generalized Autoregressive Conditional Heteroskedasticity. <em>Journal of Econometrics</em>, 
31(3), 307-327.
</div>

<div class="reference">
[7] Sims, C. A. (1980). Macroeconomics and Reality. <em>Econometrica</em>, 48(1), 1-48.
</div>

<div class="reference">
[8] Cohen, J. (1988). <em>Statistical Power Analysis for the Behavioral Sciences</em> (2nd ed.). 
Lawrence Erlbaum Associates.
</div>

<div class="reference">
[9] Diebold, F. X., & Mariano, R. S. (1995). Comparing Predictive Accuracy. <em>Journal of Business & 
Economic Statistics</em>, 13(3), 253-263.
</div>

<div class="reference">
[10] LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep Learning. <em>Nature</em>, 521(7553), 436-444.
</div>

<hr style="margin-top: 30pt; margin-bottom: 20pt;">

<div class="footnote">
<strong>Data Availability:</strong> Code and processed data available upon request. Raw stock data sourced from 
Yahoo Finance. SEC filings from EDGAR database (public). Contact: [Your contact information]
</div>

<div class="footnote">
<strong>Acknowledgments:</strong> This research was conducted as an independent study exploring the application 
of Graph Neural Networks to financial markets. Special thanks to the open-source community for PyTorch, 
pandas, and scientific Python ecosystem.
</div>

<div class="footnote">
<strong>Conflict of Interest:</strong> The author declares no financial conflicts of interest related to this research.
</div>

<div class="footnote" style="text-align: center; margin-top: 30pt;">
<em>End of Research Paper</em><br>
Generated: December 27, 2024
</div>

</body>
</html>